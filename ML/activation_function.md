# 활성화 함수(Activation Function)

- 인공 신경망에서 뉴런의 출력 값을 결정하는 데 사용되는 함수
- 입력 신호를 비선형적으로 변환하여 다음 계층으로 전달함

## 활성화의 의미

### 생물학적 배경:

- 생물학적 신경망의 작동 방식에서 영감을 받아 붙여짐.
- 뉴런의 활성화: 생물학적 뉴런은 입력 자극을 받으면 일정한 임계값(Threshold)을 넘는 경우에만 활성화되어 **신호(출력)** 를 전달함.
- 이 "활성화" 과정을 모방하여, 인공 신경망에서도 입력을 처리한 후 특정 조건에 따라 출력 신호를 생성하도록 설계됨

### 인공 신경망

1. 출력을 결정하는 역할:

- 입력값을 기반으로 뉴런이 "활성화"될지 말지를 결정
- 활성화 함수는 뉴런이 다음 계층으로 어떤 신호를 보낼지를 수학적으로 정의

2. 조건적 동작:

- 특정 임계값 이상에서만 뉴런이 활성화되는 ReLU나, 출력값이 특정 범위로 제한되는 Sigmoid와 같은 함수는 생물학적 활성화 메커니즘을 모방한 것

3. 비선형적 학습 가능성 부여:

- 뉴런의 활성화 상태에 따라 네트워크가 복잡한 패턴을 학습할 수 있는 비선형성을 추가함.

### "활성화" 용어의 상징성

- 활성화 = 신호 전달: 뉴런이 "활성화"되었다는 것은, 해당 뉴런이 유의미한 신호를 계산하여 다음 단계로 전달한다는 의미
- 결정적 역할: 활성화 함수는 입력을 단순히 처리하는 것이 아니라, 결과를 해석하여 모델의 최종 출력을 결정하는 중요한 역할

## 주요 역할

1. 비선형성 부여

- 신경망이 선형 변환만 수행하면 복잡한 문제를 학습할 수 없음. 비선형성을 추가하여 신경망이 복잡한 패턴과 데이터 구조를 학습하도록 만듦

2. 출력값 범위 제한

- 출력값의 범위를 제한하여 값이 지나치게 크거나 작아지는 것을 방지함. 이를 통해 학습이 안정적으로 진행됨.

3. 계산 효율성

- 역전파 과정에서 미분 가능해야 하며, 계산량이 너무 크지 않아야 효율적인 학습이 가능함.

## 자주 사용되는 활성화 함수

1. Sigmoid
2. Tanh (Hyperbolic Tangent)
3. ReLU (Rectified Linear Unit)
4. Leaky ReLU
5. Softmax
6. Swish
